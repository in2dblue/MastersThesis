[config]
dataset = fcepublic
path_train = modeldata/train_with_features.tsv
path_dev = modeldata/News_Dev_final.tsv,modeldata/WikiNews_Dev_final.tsv,modeldata/Wikipedia_Dev_final.tsv
path_test = modeldata/News_Test_final.tsv:modeldata/WikiNews_Test_final.tsv:modeldata/Wikipedia_Test_final.tsv:modeldata/INSCRIPT_TESTDATA_final_token.tsv:modeldata/ONESTOP_TESTDATA_final_token.tsv
conll_eval = False
main_label = C
model_selector = dev_f05:high
preload_vectors_orig = embeddings/glove/glove.6B.300d.txt
preload_vectors = embeddings/glove/crawl-300d-2M.vec
word_embedding_size = 300
crf_on_top = False
emb_initial_zero = False
train_embeddings = True
char_embedding_size = 100
word_recurrent_size = 300
char_recurrent_size = 100
hidden_layer_size = 50
char_hidden_layer_size = 50
lowercase = True
replace_digits = True
min_word_freq = -1
singletons_prob = 0.1
allowed_word_length = -1
max_train_sent_length = -1
vocab_include_devtest = True
vocab_only_embedded = False
initializer = glorot
opt_strategy = adadelta
learningrate = 1.0
clip = 0.0
batch_equal_size = False
max_batch_size = 32
epochs = 200
stop_if_no_improvement_for_epochs = 7
learningrate_decay = 0.9
dropout_input = 0.5
dropout_word_lstm = 0.5
tf_per_process_gpu_memory_fraction = 1.0
tf_allow_growth = True
main_cost = 1.0
lmcost_max_vocab_size = 7500
lmcost_hidden_layer_size = 50
lmcost_lstm_gamma = 0.1
lmcost_joint_lstm_gamma = 0.0
lmcost_char_gamma = 0.0
lmcost_joint_char_gamma = 0.0
char_attention_cosine_cost = 1.0
char_integration_method = concat
save = cwi_seq_f.model
load = 
garbage_collection = False
lstm_use_peepholes = False
random_seed = 100
